{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Frozen Lake Environment from OpenAI Gym\n",
    "## Define the Placeholder for the Input Data and Variables for the Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "env = gym.make('FrozenLake-v0')\n",
    "#The grid with 4x4 gives 16 possible states, hence we have an array of 16 states.\n",
    "inputs = tf.placeholder(shape=[1,16],dtype=tf.float32)\n",
    "#Each state has 4 possible outcomes, hence we have 16x4 matrix with weights uniformly distributed\n",
    "weights = tf.Variable(tf.random_uniform([16,4],0,0.1))\n",
    "#Find the dot product of inputs and the weights\n",
    "Q1 = tf.matmul(inputs,weights)\n",
    "#The next state will be the opted based on the argmax function.\n",
    "output = tf.argmax(Q1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Next State and Error Correction Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2 = tf.placeholder(shape=[1,4],dtype=tf.float32)\n",
    "loss = tf.reduce_sum(tf.square(Q2 - Q1))\n",
    "gdo = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "updatedweights = gdo.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 57 was successful, Agent reached the Goal\n",
      "Episode 68 was successful, Agent reached the Goal\n",
      "Episode 80 was successful, Agent reached the Goal\n",
      "Episode 96 was successful, Agent reached the Goal\n",
      "Episode 112 was successful, Agent reached the Goal\n",
      "Episode 122 was successful, Agent reached the Goal\n",
      "Episode 136 was successful, Agent reached the Goal\n",
      "Episode 151 was successful, Agent reached the Goal\n",
      "Episode 153 was successful, Agent reached the Goal\n",
      "Episode 155 was successful, Agent reached the Goal\n",
      "Episode 164 was successful, Agent reached the Goal\n",
      "Episode 180 was successful, Agent reached the Goal\n",
      "Episode 183 was successful, Agent reached the Goal\n",
      "Episode 210 was successful, Agent reached the Goal\n",
      "Episode 224 was successful, Agent reached the Goal\n",
      "Episode 227 was successful, Agent reached the Goal\n",
      "Episode 228 was successful, Agent reached the Goal\n",
      "Episode 233 was successful, Agent reached the Goal\n",
      "Episode 242 was successful, Agent reached the Goal\n",
      "Episode 248 was successful, Agent reached the Goal\n",
      "Episode 251 was successful, Agent reached the Goal\n",
      "Episode 254 was successful, Agent reached the Goal\n",
      "Episode 255 was successful, Agent reached the Goal\n",
      "Episode 258 was successful, Agent reached the Goal\n",
      "Episode 265 was successful, Agent reached the Goal\n",
      "Episode 280 was successful, Agent reached the Goal\n",
      "Episode 287 was successful, Agent reached the Goal\n",
      "Episode 289 was successful, Agent reached the Goal\n",
      "Episode 290 was successful, Agent reached the Goal\n",
      "Episode 297 was successful, Agent reached the Goal\n",
      "Episode 302 was successful, Agent reached the Goal\n",
      "Episode 307 was successful, Agent reached the Goal\n",
      "Episode 340 was successful, Agent reached the Goal\n",
      "Episode 349 was successful, Agent reached the Goal\n",
      "Episode 353 was successful, Agent reached the Goal\n",
      "Episode 370 was successful, Agent reached the Goal\n",
      "Episode 375 was successful, Agent reached the Goal\n",
      "Episode 380 was successful, Agent reached the Goal\n",
      "Episode 381 was successful, Agent reached the Goal\n",
      "Episode 409 was successful, Agent reached the Goal\n",
      "Episode 412 was successful, Agent reached the Goal\n",
      "Episode 416 was successful, Agent reached the Goal\n",
      "Episode 420 was successful, Agent reached the Goal\n",
      "Episode 423 was successful, Agent reached the Goal\n",
      "Episode 430 was successful, Agent reached the Goal\n",
      "Episode 440 was successful, Agent reached the Goal\n",
      "Episode 472 was successful, Agent reached the Goal\n",
      "Episode 495 was successful, Agent reached the Goal\n",
      "Episode 499 was successful, Agent reached the Goal\n",
      "Episode 506 was successful, Agent reached the Goal\n",
      "Episode 508 was successful, Agent reached the Goal\n",
      "Episode 525 was successful, Agent reached the Goal\n",
      "Episode 533 was successful, Agent reached the Goal\n",
      "Episode 555 was successful, Agent reached the Goal\n",
      "Episode 562 was successful, Agent reached the Goal\n",
      "Episode 594 was successful, Agent reached the Goal\n",
      "Episode 602 was successful, Agent reached the Goal\n",
      "Episode 609 was successful, Agent reached the Goal\n",
      "Episode 612 was successful, Agent reached the Goal\n",
      "Episode 614 was successful, Agent reached the Goal\n",
      "Episode 618 was successful, Agent reached the Goal\n",
      "Episode 619 was successful, Agent reached the Goal\n",
      "Episode 627 was successful, Agent reached the Goal\n",
      "Episode 630 was successful, Agent reached the Goal\n",
      "Episode 634 was successful, Agent reached the Goal\n",
      "Episode 636 was successful, Agent reached the Goal\n"
     ]
    }
   ],
   "source": [
    "gamma = 0.9\n",
    "epsilon = 0.1\n",
    "episodes = 2000\n",
    "\n",
    "totalReward = 0\n",
    "\n",
    "session = tf.Session()\n",
    "session.run(tf.initialize_all_variables())\n",
    "for i in range(episodes):\n",
    "    state_now = env.reset()\n",
    "    done = False\n",
    "    reward = 0\n",
    "    for j in range(100):\n",
    "        #Lets find the dot product of the inputs with the weights and apply argmax on it.\n",
    "        action , Y = session.run([output, Q1], feed_dict = {inputs : [np.eye(16)[state_now]]})\n",
    "        if epsilon > np.random.rand(1):\n",
    "            action[0] = env.action_space.sample()\n",
    "            epsilon -= 10**-3\n",
    "        #Lets iterate to the next state Note: This can be random.\n",
    "        state_next , reward, done, _ = env.step(action[0])\n",
    "        Y1 = session.run(Q1, feed_dict = {inputs : [np.eye(16)[state_next]]})\n",
    "        change_Y = Y\n",
    "        change_Y[0, action[0]] = reward + gamma*np.max(Y1)\n",
    "        #Updating the weights \n",
    "        _,new_weights = session.run([updatedweights,weights],feed_dict={inputs:[np.eye(16)[state_now]],Q2:change_Y})\n",
    "        #Lets append the total number of rewards\n",
    "        totalReward += reward\n",
    "        state_now = state_next\n",
    "        if reward == 1:\n",
    "            print ('Episode {} was successful, Agent reached the Goal'.format(i))\n",
    "            \n",
    "session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
